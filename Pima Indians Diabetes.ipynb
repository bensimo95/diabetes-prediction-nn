{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a86832b7-feb6-493d-bcb4-8e4f68ff5a3b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h2>Pima Indians Diabetes traitement</h2>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d51ba5-716b-476c-935e-1ae0e4ffc982",
   "metadata": {},
   "source": [
    "#### Titre : 'Pima Indians Diabetes Database'\n",
    "La variable de diagnostic, d'une valeur binaire d'une enquête est de savoir si le patient présente des signes de diabète selon l’OMS.\n",
    "#### Sources :\n",
    "Propriétaires d'origine : National Institute of Diabetes and Digestive and Kidney Diseases.\n",
    "#### Informations pertinentes :\n",
    "- En particulier, tous les patients sont des femmes d'au moins 21 ans d'origine indienne Pima.\n",
    "- Nombre d'instances : 768\n",
    "- Nombre d'attributs : 8 plus la classe\n",
    "#### Les attributs : (tous à valeur numérique)\n",
    "- Nombre de grossesses (Pregnancies).\n",
    "- Taux de glucose (Glucose) : Concentration de glucose plasmatique à 2 heures dans un test de tolérance au glucose par voie orale.\n",
    "- Pression artérielle diastolique en mm Hg (BloodPressure).\n",
    "- Épaisseur du pli cutané du triceps en mm (SkinThickness).\n",
    "- Taux d'insuline sérique sur 2 heures (Insulin).\n",
    "- Indice de masse corporelle en (poids en kg/(taille en m)^2) (BMI).\n",
    "- Fonction de pedigree du diabète (DiabetesPedigreeFunction).\n",
    "- Âge en ans (Age).\n",
    "- Résultat (Présence de diabète ou non) (Outcome) : Catégoriel (0 ou 1).\n",
    "#### Distribution de classe : \n",
    "La valeur de classe 1 est interprétée comme \"testé positif pour le diabète\".\n",
    "#### Valeur de classe Nombre d'instances\n",
    "- 0 (test négatif) : 500\n",
    "- 1 (test positif) : 268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2fccc-7d8c-4f90-a0ae-e8c6503cbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40c4c5-2848-4002-92f5-0a0808d40c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'Pima Indians Diabetes Database\\diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b7672-623b-4d9d-a150-4bd43bd79bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65c301-2170-4ed3-97a7-09a3239ce03b",
   "metadata": {},
   "source": [
    "# compter les valeurs de 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09199fd9-cb2d-4c81-84fd-56027c20e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.loc[:,:]==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34ebaf-fd59-4e7d-b500-437a473c52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfe7281-012e-43b2-ada4-7ab4c2e0468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d32251-93ee-49ba-a7e0-35d255700e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35c328-d3fd-46fe-9b13-ecb4caaca2d9",
   "metadata": {},
   "source": [
    "# afficher les valeurs aberantes avec boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2e964-8246-40b0-b190-07c50189e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot()\n",
    "plt.title(\"Boxplot des variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9ec9f-fa50-4fcc-88ff-eb39b5a19a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in df.columns :\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Définir les limites pour les valeurs aberrantes\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Identifier les valeurs aberrantes\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "\n",
    "    # Remplacer les valeurs aberrantes par les limites\n",
    "    df[col] = df[col].clip(lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688830b7-761f-475f-9acc-285887d64b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd370da8-9d5f-4dcc-9dad-35f7067f28db",
   "metadata": {},
   "source": [
    "# remplacer 0 par la median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0f053-d67e-4972-8749-85c39b0b34cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_ivalid_zero = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "for i in cols_ivalid_zero :\n",
    "    median = df[i].median()\n",
    "    df[i] = df[i].replace(0,median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf389c-d9f0-48f1-9f2a-2b9d1e52804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.loc[:,:]==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e3a8b-a7cc-4ea6-8dd7-c43626920db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b346ee-0a36-4450-a205-05df25c32f91",
   "metadata": {},
   "source": [
    "# data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fafdad0-b3b9-4239-90b3-dcc0b581d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "#separation de X et y\n",
    "X = df.drop('Outcome',axis = 1)\n",
    "y = df['Outcome']\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2 ,stratify = y ,random_state = 11 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1381a7-c07b-4d69-b4a5-39bcaaefe26c",
   "metadata": {},
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f40499c-0c9f-475e-996d-8ad8b314b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------------\n",
    "# Fonctions d’activation\n",
    "# -------------------------------\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation : max(0, x)\n",
    "    \"\"\"\n",
    "    assert isinstance(x, np.ndarray), \"Input to ReLU must be a numpy array\"\n",
    "    result = np.maximum(0, x)\n",
    "    assert np.all(result >= 0), \"ReLU output must be non-negative\"\n",
    "    return result\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU: 1 if x > 0, else 0\n",
    "    \"\"\"\n",
    "    assert isinstance(x, np.ndarray), \"Input to ReLU derivative must be a numpy array\"\n",
    "    result = np.where(x > 0, 1, 0)\n",
    "    assert np.all((result == 0) | (result == 1)), \"ReLU derivative must be 0 or 1\"\n",
    "    return result\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation : 1 / (1 + exp(-x))\n",
    "    \"\"\"\n",
    "    assert isinstance(x, np.ndarray), \"Input to sigmoid must be a numpy array\"\n",
    "    result = 1 / (1 + np.exp(-x))\n",
    "    assert np.all((result >= 0) & (result <= 1)), \"Sigmoid output must be in [0, 1]\"\n",
    "    return result\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid: sigmoid(x) * (1 - sigmoid(x))\n",
    "    \"\"\"\n",
    "    assert isinstance(x, np.ndarray), \"Input to sigmoid derivative must be a numpy array\"\n",
    "    s = sigmoid(x)\n",
    "    result = s * (1 - s)\n",
    "    assert np.all((result >= 0) & (result <= 0.25)), \"Sigmoid derivative must be in [0, 0.25]\"\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da21b1-3b6a-4f77-a1a2-b292d9b9a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Calcul des poids de classes à partir de y_train\n",
    "weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                             classes=np.unique(y_train),\n",
    "                                             y=y_train.ravel())\n",
    "class_weights_dict = {0: weights[0], 1: weights[1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5b83f-6e7c-4db6-8339-ea78d4321016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01, l2_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with given layer sizes and learning rate.\n",
    "        layer_sizes : list of int [input_size, hidden1_size, ..., output_size]\n",
    "        learning_rate : float\n",
    "        l2_lambda : float, coefficient for L2 regularization\n",
    "        \"\"\"\n",
    "        assert isinstance(layer_sizes, list) and len(layer_sizes) >= 2, \"layer_sizes must be a list with at least 2 elements\"\n",
    "        assert all(isinstance(size, int) and size > 0 for size in layer_sizes), \"All layer sizes must be positive integers\"\n",
    "        assert isinstance(learning_rate, (int, float)) and learning_rate > 0, \"Learning rate must be positive\"\n",
    "        assert isinstance(l2_lambda, (int, float)) and l2_lambda >= 0, \"L2 regularization must be non-negative\"\n",
    "\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        np.random.seed(42)\n",
    "        # Initialize weights with Xavier initialization and biases with zeros\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / layer_sizes[i])\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            assert w.shape == (layer_sizes[i], layer_sizes[i + 1]), f\"Weight matrix {i + 1} has incorrect shape\"\n",
    "            assert b.shape == (1, layer_sizes[i + 1]), f\"Bias vector {i + 1} has incorrect shape\"\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation:\n",
    "        Z^{[l]} = A^{[l-1]} W^{[l]} + b^{[l]}\n",
    "        A^{[l]} = g(Z^{[l]}) with sigmoid activation\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray), \"Input X must be a numpy array\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "\n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            assert z.shape == (X.shape[0], self.layer_sizes[i + 1]), f\"Z^{i + 1} has incorrect shape\"\n",
    "            self.z_values.append(z)\n",
    "            a = sigmoid(z)\n",
    "            self.activations.append(a)\n",
    "\n",
    "        assert self.activations[-1].shape == (X.shape[0], self.layer_sizes[-1]), \"Output activation has incorrect shape\"\n",
    "        return self.activations[-1]\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Binary Cross-Entropy loss:\n",
    "        J = -1/m * sum(y*log(y_pred) + (1-y)*log(1-y_pred))\n",
    "        \"\"\"\n",
    "        assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray), \"Inputs to loss must be numpy arrays\"\n",
    "        assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
    "        assert np.all((y_true == 0) | (y_true == 1)), \"y_true must contain only 0s and 1s\"\n",
    "\n",
    "        m = y_true.shape[0]\n",
    "        # Add small epsilon to avoid log(0)\n",
    "        epsilon = 1e-8\n",
    "        loss = -np.sum(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon)) / m\n",
    "\n",
    "        # Add L2 regularization loss\n",
    "        if self.l2_lambda > 0:\n",
    "            l2_loss = 0.5 * self.l2_lambda * sum(np.sum(w ** 2) for w in self.weights)\n",
    "            loss += l2_loss / m\n",
    "\n",
    "        assert not np.isnan(loss), \"Loss computation resulted in NaN\"\n",
    "        return loss\n",
    "\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute accuracy : proportion of correct predictions\n",
    "        \"\"\"\n",
    "        assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray), \"Inputs to accuracy must be numpy arrays\"\n",
    "        assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
    "\n",
    "        preds = (y_pred >= 0.5).astype(int)\n",
    "        accuracy = np.mean(preds == y_true)\n",
    "        assert 0 <= accuracy <= 1, \"Accuracy must be between 0 and 1\"\n",
    "        return accuracy\n",
    "\n",
    "    def backward(self, X, y, outputs):\n",
    "        \"\"\"\n",
    "        Backpropagation : compute gradients dW^{[l]}, db^{[l]} for each layer\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray) and isinstance(outputs, np.ndarray), \"Inputs to backward must be numpy arrays\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y.shape == outputs.shape, \"y and outputs must have the same shape\"\n",
    "\n",
    "        m = X.shape[0]\n",
    "        self.d_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.d_biases = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Output layer error\n",
    "        dZ = outputs - y\n",
    "        assert dZ.shape == outputs.shape, \"dZ for output layer has incorrect shape\"\n",
    "\n",
    "        self.d_weights[-1] = (self.activations[-2].T @ dZ) / m\n",
    "        self.d_biases[-1] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Backpropagate through hidden layers\n",
    "        for i in range(len(self.weights) - 2, -1, -1):\n",
    "            dA = dZ @ self.weights[i + 1].T\n",
    "            dZ = dA * sigmoid_derivative(self.activations[i + 1])\n",
    "            assert dZ.shape == (m, self.layer_sizes[i + 1]), f\"dZ at layer {i + 1} has incorrect shape\"\n",
    "            self.d_weights[i] = (self.activations[i].T @ dZ) / m\n",
    "\n",
    "            # Add L2 regularization to weight gradients\n",
    "            if self.l2_lambda > 0:\n",
    "                self.d_weights[i] += (self.l2_lambda / m) * self.weights[i]\n",
    "\n",
    "            self.d_biases[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * self.d_weights[i]\n",
    "            self.biases[i] -= self.learning_rate * self.d_biases[i]\n",
    "\n",
    "    def train(self, X, y, X_val, y_val, epochs, batch_size):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch SGD, with validation\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray), \"X and y must be numpy arrays\"\n",
    "        assert isinstance(X_val, np.ndarray) and isinstance(y_val, np.ndarray), \"X_val and y_val must be numpy arrays\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y.shape[1] == self.layer_sizes[-1], f\"Output dimension ({y.shape[1]}) must match output layer size ({self.layer_sizes[-1]})\"\n",
    "        assert X_val.shape[1] == self.layer_sizes[0], f\"Validation input dimension ({X_val.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y_val.shape[1] == self.layer_sizes[-1], f\"Validation output dimension ({y_val.shape[1]}) must match output layer size ({self.layer_sizes[-1]})\"\n",
    "        assert isinstance(epochs, int) and epochs > 0, \"Epochs must be a positive integer\"\n",
    "        assert isinstance(batch_size, int) and batch_size > 0, \"Batch size must be a positive integer\"\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "\n",
    "                outputs = self.forward(X_batch)\n",
    "                epoch_loss += self.compute_loss(y_batch, outputs)\n",
    "                self.backward(X_batch, y_batch, outputs)\n",
    "\n",
    "            train_loss = epoch_loss / (X.shape[0] / batch_size)\n",
    "\n",
    "            # Validation loss and accuracy\n",
    "            val_outputs = self.forward(X_val)\n",
    "            val_loss = self.compute_loss(y_val, val_outputs)\n",
    "\n",
    "            train_accuracy = self.compute_accuracy(y, self.forward(X))\n",
    "            val_accuracy = self.compute_accuracy(y_val, val_outputs)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels (0 or 1)\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray), \"Input X must be a numpy array\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "\n",
    "        outputs = self.forward(X)\n",
    "        predictions = (outputs >= 0.5).astype(int)\n",
    "        assert predictions.shape == (X.shape[0], self.layer_sizes[-1]), \"Predictions have incorrect shape\"\n",
    "        return predictions\n",
    "\n",
    "# Exemple d'utilisation avec le dataset diabetes.csv\n",
    "\n",
    "# Charger et préparer les données\n",
    "data = df.copy()\n",
    "\n",
    "# Séparer les features et la cible\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "assert X.shape[0] == y.shape[0], \"Number of samples in X and y must match\"\n",
    "assert X.shape[1] == 8, \"Expected 8 features in input data\"\n",
    "\n",
    "# Standardisation : (X - mu) / sigma\n",
    "mu = X.mean(axis=0)\n",
    "sigma = X.std(axis=0)\n",
    "X = (X - mu) / sigma\n",
    "\n",
    "# Diviser les données en train, validation et test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "assert X_train.shape[0] + X_val.shape[0] + X_test.shape[0] == X.shape[0], \"Train-val-test split sizes must sum to total samples\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78648375-a8b6-4df2-bd2f-93159a1ce006",
   "metadata": {},
   "source": [
    "# entrainement avec learning rate 0.01  l2_lambda=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37505d-9573-4676-b929-9ceca6a47c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et entraîner le modèle ... learning rate 0.01\n",
    "layer_sizes = [X_train.shape[1], 16, 8, 1]\n",
    "nn = NeuralNetwork(layer_sizes, learning_rate=0.01, l2_lambda=0.001)\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(X_train, y_train, X_val, y_val, epochs=100, batch_size=32)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = nn.predict(X_test)\n",
    "print(\"\\nRapport de classification (Test set) :\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d63e986-11f9-49f5-ab94-d00522e8b195",
   "metadata": {},
   "source": [
    "# entrainement avec learning rate 0.1  l2_lambda=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f129498-296c-4a82-8e76-9f9c158207f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et entraîner le modèle ... learning rate 0.1\n",
    "layer_sizes = [X_train.shape[1], 16, 8, 1]\n",
    "nn = NeuralNetwork(layer_sizes, learning_rate=0.1, l2_lambda=0.001)\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(X_train, y_train, X_val, y_val, epochs=100, batch_size=32)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = nn.predict(X_test)\n",
    "print(\"\\nRapport de classification (Test set) :\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb0952-12a6-4ce6-8f20-b56ade70b42c",
   "metadata": {},
   "source": [
    "# entrainement avec learning rate 1  l2_lambda=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e50b4a8-e71e-446d-ba54-5af8cf21a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et entraîner le modèle ... learning rate 1\n",
    "layer_sizes = [X_train.shape[1], 16, 8, 1]\n",
    "nn = NeuralNetwork(layer_sizes, learning_rate=1, l2_lambda=0.001)\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(X_train, y_train, X_val, y_val, epochs=100, batch_size=32)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = nn.predict(X_test)\n",
    "print(\"\\nRapport de classification (Test set) :\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9fc9a0-3950-4c06-98c1-5e14d97b04dc",
   "metadata": {},
   "source": [
    "# entrainement avec learning rate 1  l2_lambda=0.007 avec 1 couche "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe35a0b-e0c3-4e35-a906-a40579bc04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et entraîner le modèle ... learning rate 1\n",
    "layer_sizes = [X_train.shape[1], 8, 1]\n",
    "nn = NeuralNetwork(layer_sizes, learning_rate=1, l2_lambda=0.007)\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(X_train, y_train, X_val, y_val, epochs=100, batch_size=32)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = nn.predict(X_test)\n",
    "print(\"\\nRapport de classification (Test set) :\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745b14e-352e-4d18-a3a7-b095f23c76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et entraîner le modèle ... learning rate 1\n",
    "layer_sizes = [X_train.shape[1], 16, 8, 1]\n",
    "nn = NeuralNetwork(layer_sizes, learning_rate=1, l2_lambda=0.007)\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(X_train, y_train, X_val, y_val, epochs=100, batch_size=32)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = nn.predict(X_test)\n",
    "print(\"\\nRapport de classification (Test set) :\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc996d0-c381-4512-a120-77b83a3239fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et entraîner le modèle ... learning rate 1\n",
    "layer_sizes = [X_train.shape[1], 17, 8, 1]\n",
    "nn = NeuralNetwork(layer_sizes, learning_rate=1, l2_lambda=0.007)\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(X_train, y_train, X_val, y_val, epochs=100, batch_size=32)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = nn.predict(X_test)\n",
    "print(\"\\nRapport de classification (Test set) :\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b67e1-9bae-48e0-985a-ef2e1e72912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et entraîner le modèle\n",
    "layer_sizes = [X_train.shape[1], 16, 8, 1]\n",
    "nn = NeuralNetwork(layer_sizes, learning_rate=1, l2_lambda=0.007)\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(X_train, y_train, X_val, y_val, epochs=100, batch_size=32)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = nn.predict(X_test)\n",
    "print(\"\\nRapport de classification (Test set) :\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Prédiction')\n",
    "plt.ylabel('Vérité terrain')\n",
    "plt.xticks([0,1])\n",
    "plt.yticks([0,1])\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i,j], ha='center', va='center', color='red')\n",
    "plt.show()\n",
    "\n",
    "# Courbes de perte et d'accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Courbe de perte\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(val_losses, label='Validation Loss')\n",
    "ax1.set_title(\"Loss au cours des epochs\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "\n",
    "# Courbe d'accuracy\n",
    "ax2.plot(train_accuracies, label='Train Accuracy')\n",
    "ax2.plot(val_accuracies, label='Validation Accuracy')\n",
    "ax2.set_title(\"Accuracy au cours des epochs\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
